---
title: 2023-10-15周报
date: 2023-10-15 18:17:27
tags:
---
本周的主要学习任务是准备数据集、寻找方法对图像进行压缩降维等。首先是数据集的准备，目前就准备了两个类别--jsk（jumper skirt）和op（one piece）。这里需要说明一下，jsk是指无袖连衣裙，而op则相反，是指有袖连衣裙。每个类别里都放了差不多150张图片。
![](./2023-10-15周报/op%20and%20jsk.jpg)
## 使用k-means算法降低分辨率
之前学习过的k-means算法可以用于降低分辨率，因此我尝试使用k-means算法进行图像处理。接下来结合代码讲解我的思路：
构造一个函数，传入的参数data即我们要进行处理的图像，返回值image则是已经进行过处理的图像。
```py
def kmeans_scikit(data):
    pic=data
    data=data.convert("RGB")
    # 将图像转换为NumPy数组
    image_array = np.array(data)
    
    # 正则化处理
    normalized_image = (image_array - np.mean(image_array)) / np.std(image_array)
    
    
    data = np.reshape(normalized_image,(250*333, 3))
    data.shape
    #导入k-means库 /k-meansライブラリのインポート
    from sklearn.cluster import KMeans
    # 构建kmeans算法模型 /kmeansアルゴリズムのモデル化
    model = KMeans(n_clusters=13, n_init=2000)
    # 开始训练 /トレーニング開始
    model.fit(data)
    centroids = model.cluster_centers_  
    print(centroids.shape)              # 查看簇的形状 \クラスターの形状を見る
    C = model.predict(data)             # 获取每条数据所属簇 \各データが属するクラスタを取得する
    C.shape

    centroids[C].shape  
    compressed_pic = centroids[C].reshape((333,250,3))
    # 绘制原图和压缩图片 /オリジナル画像と圧縮画像の描画
    fig, ax = plt.subplots(1, 2)
    ax[0].imshow(pic)
    image = ax[1].imshow(compressed_pic)
    return image
```
![](./2023-10-15周报/kmeans-output.png)

## 平均池化法
平均池化（Average Pooling）是一种常用的下采样操作，通常用于卷积神经网络（CNN）中。它可以将输入特征图的空间尺寸减小，并保留主要特征。
于是我又试着用平均池化法进行处理：
```py
def downsample_image(image, scale_factor):
    # 获取原始图像的尺寸
    
    image = np.array(image)
    height, width = image.shape[:2]
    
    # 计算降低分辨率后的新尺寸
    new_height = int(height / scale_factor)
    new_width = int(width / scale_factor)
    
    # 使用最近邻插值方法进行图像缩放
    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_NEAREST)
    
    return resized_image

# 读取原始图像
data = image_array[0]
# 设置缩放因子
scale_factor = 2
```
![](./2023-10-15周报/downsample-output.png)

## PCA（Principal Component Analysis，主成分分析）
PCA是一种常用的降维算法，用于将高维数据转换为低维表示，同时保留数据中最重要的信息。PCA的基本思想是通过线性变换将原始数据投影到一个新的坐标系中，使得投影后的数据具有最大的方差。这个新的坐标系由一组相互正交的主成分构成，主成分是原始数据经过特征值分解得到的线性组合。
```py
#协方差矩阵
def Cov(dataMat):
	meanVal = np.mean(data,0) #压缩行，返回1*cols矩阵，对各列求均值
	meanVal = np.tile(meanVal, (rows,1)) #返回rows行的均值矩阵
	Z = dataMat - meanVal
	Zcov = (1/(rows-1))*Z.T * Z
	return Zcov

#数据中心化
def Z_centered(dataMat):
	rows,cols=dataMat.shape
	meanVal = np.mean(dataMat, axis=0)  # 按列求均值，即求各个特征的均值
	meanVal = np.tile(meanVal,(rows,1))
	newdata = dataMat-meanVal
	return newdata, meanVal

#最小化降维造成的损失，确定k
def Percentage2n(eigVals, percentage):
	sortArray = np.sort(eigVals)  # 升序
	sortArray = sortArray[-1::-1]  # 逆转，即降序
	arraySum = sum(sortArray)
	tmpSum = 0
	num = 0
	for i in sortArray:
		tmpSum += i
		num += 1
		if tmpSum >= arraySum * percentage:
			return num

#得到最大的k个特征值和特征向量
def EigDV(covMat, p):
	D, V = np.linalg.eig(covMat) # 得到特征值和特征向量
	k = Percentage2n(D, p) # 确定k值
	print("保留99%信息，降维后的特征个数："+str(k)+"\n")
	eigenvalue = np.argsort(D)
	K_eigenValue = eigenvalue[-1:-(k+1):-1]
	K_eigenVector = V[:,K_eigenValue]
	return K_eigenValue, K_eigenVector

#得到降维后的数据
def getlowDataMat(DataMat, K_eigenVector):
	return DataMat * K_eigenVector
 
#重构数据
def Reconstruction(lowDataMat, K_eigenVector, meanVal):
	reconDataMat = lowDataMat * K_eigenVector.T + meanVal
	return reconDataMat

#PCA算法
def PCA(data, p):
	dataMat = np.float32(np.mat(data))
	#数据中心化
	dataMat, meanVal = Z_centered(dataMat)
	#计算协方差矩阵
		#covMat = Cov(dataMat)
	covMat = np.cov(dataMat, rowvar=0)
	#得到最大的k个特征值和特征向量
	D, V = EigDV(covMat, p)
	#得到降维后的数据
	lowDataMat = getlowDataMat(dataMat, V)
	#重构数据
	reconDataMat = Reconstruction(lowDataMat, V, meanVal)
	return reconDataMat
```
![](./2023-10-15周报/pca-output.png)
PCA进行了很好的降维，我们保留了99%的主要特征，将特征维数从一开始的250个降到了100个。
一些进行降维的方法就介绍到这里了，接下来也会继续学习对图像特征进行提取和优化的方法。