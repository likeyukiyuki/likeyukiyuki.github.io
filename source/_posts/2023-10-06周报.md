---
title: 2023-10-06周报
date: 2023-10-06 00:20:46
tags:
---
本周学习了一些关于机器学习的算法，这里将对它们做一个总结和比较：
## 首先是线性回归算法（Linear Regression）：
优点：
1、原理较为简单，因此实现起来会比较容易，而且计算过程简单，因此建模速度很快。
2、十分容易理解，结果具有很好的可解释性，有利于决策分析。
缺点：
1、局限于线性数据，不适用于非线性数据
2、不适用于高度复杂的数据，容易欠拟合
3、对异常值非常敏感

适用场景：
因变量是连续值，并且与影响因素有线性关系时。例如我们有一个数据集，数据集里面有很多x和y。x与y存在某种线性关系，并且是y根据x的值改变，那么当我们想要预测在某个x值时，该x值对应的y值是多少时，就可以使用线性回归建模。

举例：
当我们有一系列房屋的面积大小x与价格y对应关系的数据时，想要预测200㎡的房间（x值）对应的价格（y值）是多少，就可以用线性回归建模。或者是我们知道某个品牌的手机价格x与销量y对应关系的数据时，想要预测某款新机的定价（x值）能有多少销量（y值）是多少，也可以使用线性回归。
## 支持向量机SVM（support vector machines）
优点：
1、SVM分类思想较简单，最终的决策函数是由少数支持向量决定的，因此可以在一定程度上避免特征空间的维数过大的问题。
2、SVM利用核函数不同可以处理分类和回归问题，线性和非线性的问题。
3、SVM进行二分类时效果很好
4、SVM可以利用核函数向高维空间映射，因此无论特征空间是高维还是低维表现都很好
缺点：
1、适用于小样本，当样本数量过大时，训练速度会非常慢
2、传统的SVM只能进行二分类，对于多分类效果不好
3、对于核函数的选择以及参数的调整需要花很大功夫，因为它们对于样本的训练效果有着很大影响。

适用场景：
当样本数量比较小，线性可分，参数比较少时，选择linear内核的SVM模型；线性不可分，参数比较多时，选择RBF内核的SVM模型，需要调参哦。其实根据选择的核函数不同，SVM的适用场景也有很多，如果是使用监督学习的话，样本合适的情况下，都可以尝试一下SVM。

举例：
比如判断一个邮件是否是垃圾邮件，如果我们拥有一些正常的邮件和垃圾邮件，就可以提取某些特征使用SVM进行建模，最后得到的模型就可以对邮件进行分类。
## k-means
优点：
1、原理较简单，实现容易，收敛速度快，同时可解释度也比较强
2、聚类的效果很好
3、需要调节的参数很少，只需要调节簇数k
缺点：
1、虽然仅仅需要调节簇数k，但是k值不是可以随便选取的
2、使用迭代只能得到局部最优解，不一定能得到全局最优解
3、对于噪音和异常点比较敏感
4、初始聚类中心的选择有很大的影响
5、只能用于凸形簇

适用场景：
并且没有合适的标签输入，可数值化的较大数据集，由于k-means是无监督学习所以不需要输入标签，而k-means的速度很快，所以即使是大数据集也能较快的建立模型。

举例：
k-means可以用于新闻分类，当我们拥有大量的新闻文章时，可以使用k-means将其分为不同主题。或者通过收入、消费习惯等等，将客户分为不同的群体，以便进行更好的服务。

## KNN（K-Nearest Neighbors）
优点：
1、比较适合用于多分类问题、但也可以用于回归问题
2、KNN是一个非参数化算法，因此适用于各种类型的数据
3、训练速度快，精度高
4、对异常值不敏感
缺点：
1、当特征数很多时，计算量很大，计算复杂度高
2、样本不平衡时，对于较少样本类别的预测不太准确
3、需要大量的存储空间

适用场景：KNN适用于较小的数据集，因为其原理计算复杂程度很高。可以用于非线性的数据集，但是不太适用于数据不平均的情况。

举例：
KNN的实际应用举例其实与k-means较为相似，由于都主要解决分类/聚类。而KNN是一种监督学习算法，所以需要输入标签，这是一个本质的差异。KNN也可以进行客户分类、图像分类等等。


## 四种算法对比
从解决问题方面来说
回归：线性回归、SVM、KNN
分类：SVM、KNN
聚类：k-means

接着是数据是否线性
线性：线性回归、SVM、KNN
非线性：SVM、KNN

数据集的大小
大数据集：线性回归
小数据集：SVM、KNN

以上是从三个不同方面对比，但是具体使用哪个算法进行建模，还是需要根据具体的问题进行分析后选择的。